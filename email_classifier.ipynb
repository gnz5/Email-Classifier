{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This script must be run in the same directory as the data folder containing the test and train folders. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# In a python interpreter run the following:\n",
    "# >>> nltk.download()\n",
    "\n",
    "# code folding extension:\n",
    "# pip install jupyter_contrib_nbextensions\n",
    "# jupyter contrib nbextension install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create list of words to be excluded from feature sets\n",
    "stop_words = stopwords.words('english')\n",
    "additional_stopwords = ['', 'isbn', 'edu']\n",
    "stop_words.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'alt': {\n",
    "        'atheism': {}\n",
    "    },\n",
    "    'comp': {\n",
    "        'graphics': {},\n",
    "        'os': {\n",
    "            'ms-windows': {\n",
    "                'misc': {}\n",
    "            }\n",
    "        },\n",
    "        'sys': {\n",
    "            'ibm': {\n",
    "                'pc': {\n",
    "                    'hardware': {}\n",
    "                }\n",
    "            },\n",
    "            'mac': {\n",
    "                'hardware': {}\n",
    "            }\n",
    "        },\n",
    "        'windows': {\n",
    "            'x': {}\n",
    "        }\n",
    "    },\n",
    "    'misc': {\n",
    "        'forsale': {}\n",
    "    },\n",
    "    'rec': {\n",
    "        'autos': {},\n",
    "        'motorcycles': {},\n",
    "        'sport': {\n",
    "            'baseball': {},\n",
    "            'hockey': {}\n",
    "        }\n",
    "    },\n",
    "    'sci': {\n",
    "        'crypt': {},\n",
    "        'electronics': {},\n",
    "        'med': {},\n",
    "        'space': {}\n",
    "    },\n",
    "    'soc': {\n",
    "        'religion': {\n",
    "            'christian': {}\n",
    "        }\n",
    "    },\n",
    "    'talk': {\n",
    "        'politics': {\n",
    "            'guns': {},\n",
    "            'mideast': {},\n",
    "            'misc': {}\n",
    "        },\n",
    "        'religion': {\n",
    "            'misc': {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions to help navigate category tree\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_root(category):\n",
    "    root = categories.copy()\n",
    "    if category.find('.') == -1:\n",
    "        return {category : root[category]}\n",
    "    branches = category.split('.')\n",
    "    for branch in branches:\n",
    "        try:\n",
    "            root = root[branch]\n",
    "        except TypeError:\n",
    "            return root\n",
    "    return {branch: root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def traverse(root):\n",
    "    if root is None or len(root.keys()) == 0:\n",
    "        return\n",
    "    if not isinstance(root, dict):\n",
    "        return list(root)\n",
    "    else:\n",
    "        result = []\n",
    "        for key in root.keys():\n",
    "            parent = key\n",
    "            children = traverse(root[key])\n",
    "            if children is None:\n",
    "                result.append(parent)\n",
    "            else:\n",
    "                for child in children:\n",
    "                    result.append(parent + '.' + child)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def traverse_all():\n",
    "    result = []\n",
    "    for key in categories.keys():\n",
    "        result.extend(traverse(get_root(key)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to process raw emails and extract features\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract_body(file_path, overwrite=True):\n",
    "    if not overwrite:\n",
    "        try:\n",
    "            f = open(file_path + '.pickle', 'rb')\n",
    "            parsed_file = pickle.load(f)\n",
    "            f.close()\n",
    "            return parsed_file\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    f = open(file_path, 'r')\n",
    "    count = 0\n",
    "    while(True): \n",
    "        try:\n",
    "            line = f.readline()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        if count > 150 or line.startswith('Lines:'):\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "    document = dict()\n",
    "    for line in f.readlines():\n",
    "        alphanumeric_only = re.sub(r'\\W+', ' ', line).strip(' ').lower()\n",
    "        words = [word for word in alphanumeric_only.split(' ') if len(word) > 3 and not word.isnumeric() and word not in stop_words]\n",
    "        for word in words:\n",
    "            if word not in document.keys():\n",
    "                document[word] = 1\n",
    "            else:\n",
    "                document[word] += 1\n",
    "    f.close()\n",
    "\n",
    "    if len(document) > 0:\n",
    "        parsed_file = document\n",
    "        f = open(file_path + '.pickle', 'wb')\n",
    "        pickle.dump(parsed_file, f)\n",
    "        f.close()\n",
    "        return document\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# given a string containing words separated by periods, return the substring\n",
    "# containing the first i periods\n",
    "\n",
    "def first_i_sections(string, num_periods):\n",
    "    count = 0\n",
    "    for i in range(len(string)):\n",
    "        if count > num_periods:\n",
    "            return string[:i].strip('.')\n",
    "        if string[i] == '.':\n",
    "            count += 1 \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_documents(mode, category):\n",
    "    base_path = './data/' + mode + '/'\n",
    "    if category == 'all':\n",
    "        folder_names = traverse_all()\n",
    "    else:\n",
    "        folder_names = traverse(get_root(category))\n",
    "    documents = []\n",
    "    \n",
    "    for folder in folder_names:\n",
    "        path = base_path + folder + '/'\n",
    "        file_names = os.listdir(path)\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            file_path = path + file_name\n",
    "            document = extract_body(file_path)\n",
    "            if document is not None and len(document.keys()) > 0:\n",
    "                documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_labeled_documents(mode, category, full_category=False):\n",
    "    periods = category.count('.')\n",
    "    base_path = './data/' + mode + '/'\n",
    "    if periods > 0:\n",
    "        parent = first_i_sections(category, periods-1) + '.'\n",
    "        base_path += parent\n",
    "    else:\n",
    "        parent = ''\n",
    "    if category == 'all':\n",
    "        folder_names = traverse_all()\n",
    "    else:\n",
    "        folder_names = traverse(get_root(category))\n",
    "    documents = []\n",
    "    \n",
    "    for folder in folder_names:\n",
    "        path = base_path + folder + '/'\n",
    "        file_names = os.listdir(path)\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            file_path = path + file_name\n",
    "            document = extract_body(file_path)\n",
    "            if document is not None and len(document.keys()) > 0:\n",
    "                if category == 'all':\n",
    "                    if full_category:\n",
    "                        tag = folder\n",
    "                    else:\n",
    "                        tag = first_i_sections(folder, 0)\n",
    "                else:\n",
    "                    tag = parent + first_i_sections(folder, periods+1)         \n",
    "                documents.append((document, tag))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# given a list of document where each document is a dictionary containing the\n",
    "# number of occurences of each word, return the average frequency of each word among all documents.\n",
    "# For example, if the word \"cpu\" occurs with 0.2 frequency in document 1 and 0.3 in document 2, its\n",
    "# average frequency among both documents is 0.25.\n",
    "def get_most_frequent_words(documents):\n",
    "    dfs = []\n",
    "    for document in documents:\n",
    "        dfs.append(pd.Series(document))\n",
    "\n",
    "    avg_frequency_distribution = dfs[0].copy()\n",
    "\n",
    "    for df in dfs:\n",
    "        avg_frequency_distribution.add(df, fill_value=0)\n",
    "\n",
    "    avg_frequency_distribution /= len(dfs)\n",
    "    \n",
    "    return avg_frequency_distribution.sort_values(ascending=False)[:100].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    all_features_dict = {}\n",
    "    for feature in all_features:\n",
    "        all_features_dict[feature] = 0\n",
    "    \n",
    "    keys = all_features_dict.keys()\n",
    "    for word in document:\n",
    "        if word in keys:\n",
    "            all_features_dict[word] += 1\n",
    "    return all_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "features_per_category = 300\n",
    "try:\n",
    "    f = open('all_features.pickle', 'rb')\n",
    "    all_features = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    all_features = []\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'alt'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'comp'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'misc'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'rec'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'sci'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'soc'))[:features_per_category])\n",
    "    all_features.extend(get_most_frequent_words(get_documents('train', 'talk'))[:features_per_category])\n",
    "    f = open('all_features.pickle', 'wb')\n",
    "    pickle.dump(all_features, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_data_set(mode, category, full_category=False):\n",
    "    data_set = []\n",
    "    if full_category:\n",
    "        documents = get_labeled_documents(mode, category, full_category=True)\n",
    "    else:\n",
    "        documents = get_labeled_documents(mode, category)\n",
    "    for document in documents:\n",
    "        training_example = (document_features(document[0]), document[1])\n",
    "        data_set.append(training_example)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_nodes():\n",
    "    categories = traverse_all()\n",
    "    result = set()\n",
    "    for category in categories:\n",
    "        sub_categories = category.split('.')\n",
    "        for i in range(0, len(sub_categories)):\n",
    "            if i == 0:\n",
    "                tree_path = sub_categories[i]\n",
    "            else:\n",
    "                tree_path += '.' + sub_categories[i]\n",
    "            result.add(tree_path)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_list = get_nodes()\n",
    "nodes_list.sort()\n",
    "nodes = {}\n",
    "for node in nodes_list:\n",
    "    nodes[node] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make root classifier\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_set = get_data_set('train', 'all')\n",
    "all_test_set = get_data_set('test', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_classifier = nltk.NaiveBayesClassifier.train(all_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 53.57%\n"
     ]
    }
   ],
   "source": [
    "acc = nltk.classify.accuracy(root_classifier, all_test_set)\n",
    "print(f'Accuracy = {round(acc*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make local-level classifiers\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes.keys():\n",
    "    train_set = get_data_set('train', node)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    nodes[node] = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt                           classifier accuracy = 100.0%\n",
      "alt.atheism                   classifier accuracy = 100.0%\n",
      "comp                          classifier accuracy =  49.7%\n",
      "comp.graphics                 classifier accuracy = 100.0%\n",
      "comp.os                       classifier accuracy = 100.0%\n",
      "comp.os.ms-windows            classifier accuracy = 100.0%\n",
      "comp.os.ms-windows.misc       classifier accuracy = 100.0%\n",
      "comp.sys                      classifier accuracy =  62.2%\n",
      "comp.sys.ibm                  classifier accuracy = 100.0%\n",
      "comp.sys.ibm.pc               classifier accuracy = 100.0%\n",
      "comp.sys.ibm.pc.hardware      classifier accuracy = 100.0%\n",
      "comp.sys.mac                  classifier accuracy = 100.0%\n",
      "comp.sys.mac.hardware         classifier accuracy = 100.0%\n",
      "comp.windows                  classifier accuracy = 100.0%\n",
      "comp.windows.x                classifier accuracy = 100.0%\n",
      "misc                          classifier accuracy = 100.0%\n",
      "misc.forsale                  classifier accuracy = 100.0%\n",
      "rec                           classifier accuracy =  68.6%\n",
      "rec.autos                     classifier accuracy = 100.0%\n",
      "rec.motorcycles               classifier accuracy = 100.0%\n",
      "rec.sport                     classifier accuracy =  53.6%\n",
      "rec.sport.baseball            classifier accuracy = 100.0%\n",
      "rec.sport.hockey              classifier accuracy = 100.0%\n",
      "sci                           classifier accuracy =  53.7%\n",
      "sci.crypt                     classifier accuracy = 100.0%\n",
      "sci.electronics               classifier accuracy = 100.0%\n",
      "sci.med                       classifier accuracy = 100.0%\n",
      "sci.space                     classifier accuracy = 100.0%\n",
      "soc                           classifier accuracy = 100.0%\n",
      "soc.religion                  classifier accuracy = 100.0%\n",
      "soc.religion.christian        classifier accuracy = 100.0%\n",
      "talk                          classifier accuracy =  79.1%\n",
      "talk.politics                 classifier accuracy =  56.1%\n",
      "talk.politics.guns            classifier accuracy = 100.0%\n",
      "talk.politics.mideast         classifier accuracy = 100.0%\n",
      "talk.politics.misc            classifier accuracy = 100.0%\n",
      "talk.religion                 classifier accuracy = 100.0%\n",
      "talk.religion.misc            classifier accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "for node in nodes.keys():\n",
    "    test_set = get_data_set('test', node)\n",
    "    classifier = nodes[node]\n",
    "    acc = nltk.classify.accuracy(classifier, test_set)\n",
    "    print(f'{node}'.ljust(30, ' ') + 'classifier accuracy = ' + f'{round(acc*100, 1)}%'.rjust(6, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(document):\n",
    "    current_prediction = root_classifier.classify(document)\n",
    "\n",
    "    while True:\n",
    "        #print(current_prediction)\n",
    "        classifier = nodes[current_prediction]\n",
    "        next_prediction = classifier.classify(document)\n",
    "        if current_prediction == next_prediction:\n",
    "            break\n",
    "        else:\n",
    "            current_prediction = next_prediction\n",
    "    return current_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_set_fully_defined = get_data_set('test', 'all', full_category=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for document in all_test_set_fully_defined:\n",
    "    y_true.append(document[1])\n",
    "    y_pred.append(categorize(document[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score = 0.297\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f'f1 score = {round(f1, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
